{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cultural-bibliography",
   "metadata": {},
   "source": [
    "## import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "improving-limit",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "#from datasets import load_dataset\n",
    "#from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "knowing-pioneer",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "processed-covering",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "flush-warren",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the JSON file for reading\n",
    "with open('/mnt/prj/AJ/dock_bert/Data/0427_docking_data_all.json', 'r') as f:\n",
    "    # Load the JSON data from the file\n",
    "    docking_data_all = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "nuclear-sellers",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11884560"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docking_data_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lesser-paragraph",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_0718 = [sentence for sentence in docking_data_all if all(len(word) >= 20 for word in sentence.split())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "previous-forward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11092443"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_0718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "conceptual-brunei",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792117"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docking_data_all) - len(data_0718)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sized-warning",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "undefined-alliance",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████| 11092443/11092443 [04:13<00:00, 43760.69it/s]\n"
     ]
    }
   ],
   "source": [
    "regex = r\"\\d_\\d_[A-Za-z]:[A-Za-z]_[A-Za-z]_[A-Za-z]-[A-Za-z]_[A-Za-z0-9]{1,3}:[A-Za-z0-9]{1,3}_[A-Za-z0-9]{1,3}_[A-Za-z0-9]{1,3}\"\n",
    "data_0718_re =[]\n",
    "data_0718_err =[]\n",
    "for one_sen in tqdm(data_0718):\n",
    "    word_ftwo = [item for item in one_sen.split() if re.fullmatch(regex, item)]\n",
    "    if len(one_sen.split()) == len(word_ftwo):\n",
    "        data_0718_re.append(one_sen)\n",
    "    else:\n",
    "        data_0718_err.append(one_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "annoying-victor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11092443 10475194 617249\n"
     ]
    }
   ],
   "source": [
    "print(len(data_0718), len(data_0718_re), len(data_0718_err))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dominican-forestry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_0718) == len(data_0718_re) + len(data_0718_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "global-slovak",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_6_N:C_N_N-Y_CE2:CZ_CE2_CD2 1_4_C:N_C_N-V_CG2AVAL:CG1AVAL_CG 1_6_C:C_C_C-V_CG2AVAL:CG1AVAL_CG 1_4_N:C_N_C-V_CG2AVAL:CG1AVAL_CG 1_4_N:C_N_C-K_CD:CG_CD_CE 1_4_C:O_C_C-D_O:C_N_CA 2_1_O:C_O_C-L_CD1:CB_CD1_CD2 1_6_O:C_O_C-M_O:C_N_CA 1_6_O:N_O_C-M_CG:CB_CG_SD 2_6_O:C_O_C-M_CB:CA_CB_CG 1_4_C:N_C_C-D_OD2:OD1_OD2_CB 1_0_C:C_C_C-D_OD2:OD1_OD2_CB',\n",
       " '1_6_N:N_N_C-I_CG2:CB_CG1_CD1 1_4_C:O_C_C-Y_CD2:CE2_CD2_CG 2_1_O:C_O_C-Y_CE2:CZ_CE2_CD2 1_6_O:C_O_C-V_CG1AVAL:CA_CG1AVAL 1_3_O:C_O_C-V_CG2AVAL:CG1AVAL_CG 1_4_N:C_N_C-V_CG1AVAL:CA_CG1AVAL 2_6_O:C_O_C-V_CG1AVAL:CA_CG1AVAL 2_6_O:N_O_C-K_CD:CG_CD_CE 1_4_C:N_C_C-Q_OE1:CG_OE1_NE2 1_0_C:C_C_C-Q_OE1:CG_OE1_NE2 2_6_N:C_N_N-M_O:C_N_CA 2_4_N:C_N_C-M_O:C_N_CA 2_1_N:N_N_C-M_CG:CB_CG_SD 1_4_C:N_C_N-M_O:C_N_CA 2_6_O:C_O_C-N_OD1:CB_OD1_ND2 2_6_C:C_C_C-L_CD2:CD1_CD2_CB',\n",
       " '1_4_N:C_N_C-I_CG2:CB_CG1_CD1 2_6_C:C_C_C-V_CG2AVAL:CG1AVAL_CG 1_4_N:C_N_C-V_CG1AVAL:CA_CG1AVAL 2_1_O:C_O_C-K_CD:CG_CD_CE 2_6_O:C_O_C-Q_OE1:CG_OE1_NE2 2_3_O:C_O_C-Q_NE2:NE2_OE1_CG 1_0_C:C_C_C-D_O:C_N_CA 1_6_C:C_C_C-D_O:C_N_CA 1_1_N:N_N_C-M_O:C_N_CA 1_6_O:C_O_C-N_OD1:CB_OD1_ND2 2_4_C:N_C_N-L_CD2:CD1_CD2_CB 2_4_C:N_C_C-L_CD1:CB_CD1_CD2 2_4_C:O_C_C-C_SG:SG_CB_CA']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0718_err[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unlikely-intensity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1409366"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docking_data_all) - len(data_0718_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "molecular-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in data_0718_re[0].split():\n",
    "    if len(item) < 21:\n",
    "        print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "approved-investigator",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████| 10475194/10475194 [01:03<00:00, 165079.65it/s]\n"
     ]
    }
   ],
   "source": [
    "data_0718_re_2 = []\n",
    "data_0718_re_error2 = []\n",
    "for one_sen in tqdm(data_0718_re):\n",
    "    word_ftwo = [item for item in one_sen.split() if len(item) >= 21]\n",
    "    if len(one_sen.split()) == len(word_ftwo):\n",
    "        data_0718_re_2.append(one_sen)\n",
    "    else:\n",
    "        data_0718_re_error2.append(one_sen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "substantial-switch",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10475194"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_0718_re_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "upset-transition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_1_O:C_O_C-D_OD2:OD1_OD2_CB 2_6_C:C_C_C-K_CD:CG_CD_CE 1_1_O:C_O_C-K_CB:CA_CB_CG 2_1_O:C_O_C-K_CE:CD_CE_NZ 1_1_C:P_C_C-K_CE:CD_CE_NZ 2_1_O:O_O_O-R_CG:CB_CG_CD 2_1_O:C_O_O-R_NE:CD_NE_CZ 2_1_O:O_O_C-L_CD1:CB_CD1_CD2 1_4_C:C_C_C-N_ND2:OD1_ND2_CB 2_1_O:O_O_C-H_CE1:ND1_CE1_NE2 1_1_O:O_O_C-K_CD:CG_CD_CE 2_4_C:C_C_C-R_NH2:NH1_NH2_NE',\n",
       " '2_6_O:C_O_C-D_OD2:OD1_OD2_CB 2_1_O:C_O_C-S_OG:OG_CB_CA 2_1_O:O_O_C-K_CD:CG_CD_CE 2_6_C:C_C_C-K_CD:CG_CD_CE 2_6_O:C_O_C-N_ND2:OD1_ND2_CB 1_4_C:C_C_C-K_CE:CD_CE_NZ 1_1_O:O_O_C-R_CG:CB_CG_CD 2_1_O:O_O_C-R_NH2:NH1_NH2_NE 2_1_O:C_O_C-R_NE:CD_NE_CZ 2_1_O:O_O_O-N_ND2:OD1_ND2_CB 1_1_C:P_C_C-H_CE1:ND1_CE1_NE2 2_1_O:O_O_O-H_CE1:ND1_CE1_NE2',\n",
       " '2_1_O:O_O_O-D_OD1:CB_OD1_OD2 2_6_O:C_O_C-D_OD2:OD1_OD2_CB 1_6_O:O_O_O-D_OD2:OD1_OD2_CB 1_4_C:C_C_C-D_OD1:CB_OD1_OD2 2_1_O:O_O_O-S_OG:OG_CB_CA 1_4_C:C_C_C-K_CD:CG_CD_CE 2_1_O:O_O_C-K_CE:CD_CE_NZ 1_1_O:C_O_C-E_OE2:OE1_OE2_CG 1_1_O:O_O_C-R_NE:CD_NE_CZ 1_6_O:O_O_C-R_CG:CB_CG_CD 1_6_O:C_O_C-N_OD1:CB_OD1_ND2 2_1_O:O_O_C-H_NE2:CE1_NE2_CD2 1_1_C:P_C_C-R_NH2:NH1_NH2_NE']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0718_re_2[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "photographic-tucson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Use the json.dump() method to write the list into a file\n",
    "with open('data_0718_re_2.json', 'w') as f:\n",
    "    json.dump(data_0718_re_2, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mediterranean-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_0718_re_2.json', 'r') as f:\n",
    "    data_0718_re_2 = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "consecutive-hunger",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10475194"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_0718_re_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accessible-runner",
   "metadata": {},
   "source": [
    "## tokenization again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "stupid-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer, models, trainers, pre_tokenizers\n",
    "\n",
    "# Define the tokenizer\n",
    "#tokenizer = Tokenizer(models.BPE())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "formal-usage",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tokenizers.pre_tokenizers' has no attribute 'Regex'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 21\u001b[0m\n\u001b[1;32m     12\u001b[0m trainer \u001b[38;5;241m=\u001b[39m trainers\u001b[38;5;241m.\u001b[39mBpeTrainer(\n\u001b[1;32m     13\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50257\u001b[39m,\n\u001b[1;32m     14\u001b[0m     show_progress\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     15\u001b[0m     special_tokens\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[UNK]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[SEP]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[PAD]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[CLS]\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[MASK]\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m#tokenizer.pre_tokenizer = Split(pattern=Regex('\\s|\\n'), behavior=\"isolated\")\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpre_tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mpre_tokenizers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRegex\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms|\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     24\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mtrain(data_0718_re_2,trainer)\n\u001b[1;32m     25\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(save_bpe)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tokenizers.pre_tokenizers' has no attribute 'Regex'"
     ]
    }
   ],
   "source": [
    "# Here’s the code that generated the tokenizer that looks reasonable and has 50257 vocab size.\n",
    "# The training data file is a txt where each line is a paragraph and words are separated by space\n",
    "#tokenizer = tokenizers.Tokenizer(tokenizers.models.BPE())\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "# trainer = tokenizers.trainers.BpeTrainer(\n",
    "#     vocab_size = 50257,\n",
    "#     show_progress = True,\n",
    "#     special_tokens = [\"[UNK]\", \"[SEP]\", \"[PAD]\"]\n",
    "# )\n",
    "\n",
    "# Define the trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50257,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "#tokenizer.pre_tokenizer = Split(pattern=Regex('\\s|\\n'), behavior=\"isolated\")\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Regex('\\s|\\n')\n",
    "\n",
    "\n",
    "tokenizer.train(data_0718_re_2,trainer)\n",
    "tokenizer.save(save_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "blank-showcase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10475194"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_0718_re_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "designing-recognition",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1_1_O:C_O_C-D_OD2:OD1_OD2_CB 2_6_C:C_C_C-K_CD:CG_CD_CE 1_1_O:C_O_C-K_CB:CA_CB_CG 2_1_O:C_O_C-K_CE:CD_CE_NZ 1_1_C:P_C_C-K_CE:CD_CE_NZ 2_1_O:O_O_O-R_CG:CB_CG_CD 2_1_O:C_O_O-R_NE:CD_NE_CZ 2_1_O:O_O_C-L_CD1:CB_CD1_CD2 1_4_C:C_C_C-N_ND2:OD1_ND2_CB 2_1_O:O_O_C-H_CE1:ND1_CE1_NE2 1_1_O:O_O_C-K_CD:CG_CD_CE 2_4_C:C_C_C-R_NH2:NH1_NH2_NE'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_0718_re_2[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "sporting-record",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File name too long (os error 36)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m save_bpe \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m0718_bpe\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# Train the tokenizer\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfiles\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_0718_re_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Save the tokenizer\u001b[39;00m\n\u001b[1;32m     23\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave(save_bpe)\n",
      "\u001b[0;31mException\u001b[0m: File name too long (os error 36)"
     ]
    }
   ],
   "source": [
    "# Define the tokenizer\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "\n",
    "# Define the trainer\n",
    "trainer = trainers.BpeTrainer(\n",
    "    vocab_size=50257,\n",
    "    show_progress=True,\n",
    "    special_tokens=[\"[UNK]\", \"[SEP]\", \"[PAD]\", \"[CLS]\", \"[MASK]\"],\n",
    ")\n",
    "\n",
    "# Define pre_tokenizer\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.WhitespaceSplit()\n",
    "\n",
    "# Here data_0718_re_2 must be a list of paths to your files.\n",
    "# Also, save_bpe must be the path where you want to save the tokenizer\n",
    "data_0718_re_2 = ['data_0718_re_2.json']\n",
    "save_bpe = '0718_bpe'\n",
    "\n",
    "# Train the tokenizer\n",
    "tokenizer.train(files=data_0718_re_2, trainer=trainer)\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save(save_bpe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "olympic-imagination",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dried-physiology",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/prj/AJ/dock_bert/scripts'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "appropriate-while",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/prj/AJ/dock_bert/scripts'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "paperback-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "'/mnt/prj/AJ/dock_bert/scripts/0719_token_retrain.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-fraction",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "humanitarian-adobe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-arrival",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "objective-february",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "isolated-sapphire",
   "metadata": {},
   "outputs": [],
   "source": [
    "docking_data_30p = docking_data_all[:int(len(docking_data_all) * 0.3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "destroyed-integration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3565368"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docking_data_30p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "thirty-producer",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3565368/3565368 [00:12<00:00, 282562.49it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through the list and replace ':' and '-' with an empty string ''\n",
    "for i in tqdm(range((len(docking_data_30p)))):\n",
    "    docking_data_30p[i] = docking_data_30p[i].replace('_', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "linear-ridge",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3565368/3565368 [00:05<00:00, 643091.18it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through the list and replace ':' and '-' with an empty string ''\n",
    "for i in tqdm(range((len(docking_data_30p)))):\n",
    "    docking_data_30p[i] = docking_data_30p[i].replace(':', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "boxed-eleven",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3565368/3565368 [00:04<00:00, 747542.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# loop through the list and replace ':' and '-' with an empty string ''\n",
    "for i in tqdm(range((len(docking_data_30p)))):\n",
    "    docking_data_30p[i] = docking_data_30p[i].replace('-', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "modified-folder",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3565368/3565368 [00:14<00:00, 246998.65it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3241330"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_all=[]\n",
    "for sentence in tqdm(docking_data_30p):\n",
    "    split_sen = sentence.split(' ')\n",
    "    for word in split_sen:\n",
    "        word_all.append(word)\n",
    "len(set(docking_data_30p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "provincial-rachel",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3565368/3565368 [00:15<00:00, 234915.78it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3241330"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_all=[]\n",
    "for sentence in tqdm(docking_data_30p):\n",
    "    split_sen = sentence.split(' ')\n",
    "    for word in split_sen:\n",
    "        word_all.append(word)\n",
    "len(set(docking_data_30p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "flying-encounter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90505"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fifteen-building",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90505"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(word_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "alike-finance",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_all = word_all[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "judicial-title",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['11LCLCICG2CBCG1CD1', '11LCLCIOCNCA', '14CCCCFCD1CGCD1CE1']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_all[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "freelance-input",
   "metadata": {},
   "outputs": [],
   "source": [
    "docking_data_30p = docking_data_30p[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "therapeutic-perfume",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['16CCCSWCE3CD2CE3CZ3 16OOONGOCNCA 13CNCCGOCNCA 21OCOCGCANCAC 14CCCCGOCNCA 24CCCCGCANCAC 24CCCCGCANCAC 23NCNSGCANCAC 21OOONYOHCE1OHCE2 21LCLCAOCNCA 14CCCCYOHCE1OHCE2 23CNCCFCZCE1CZCE2 14CNCCFCE2CZCE2CD2 24CNCCHCD2NE2CD2CG',\n",
       " '24NCNCFCD1CGCD1CE1 16ONOCMOCNCA 24CCCCMOCNCA 26NNNSMOCNCA 14CCCCKCGCBCGCD 14CCCCKCECDCENZ 16CCCNKCECDCENZ 26CCCSKNZNZCECD 12SCSCICG2CBCG1CD1 11COCNACBNCAC',\n",
       " '14CCCCLCD1CBCD1CD2 16CCCNLCD1CBCD1CD2 14CCCCAOCNCA 21OOOCFCZCE1CZCE2 26CCCNCSGSGCBCA 24NNNCCSGSGCBCA 24CCCCCSGSGCBCA 21COCCCSGSGCBCA 24CCCCCSGSGCBCA 14CCCCTOG1CACG2OG1 14COCCTOG1CACG2OG1 11OOOCYOHCE1OHCE2 26OCOCYCE1CD1CE1CZ 24CCCCYCE1CD1CE1CZ 14CCCCICG2CBCG1CD1 14CCCCLCD1CBCD1CD2 14CCCCVCG2CG1CG2CA 24CCCCVCG1CACG1CG2 14CCCCACBNCAC 24NNNCAOCNCA 14CCCCLCD2CD1CD2CB 14CCCCMSDCGSDSD 26CCCCMCECGSDCG 14OCOCMSDCGSDSD 24OCOCHNE2CE1NE2CD2 26COCCHCE1ND1CE1NE2 21COCCHCE1ND1CE1NE2']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docking_data_30p[3:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "copyrighted-trace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "subtle-setting",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a file in write mode\n",
    "with open('/mnt/prj/AJ/dock_bert/Data/docking_data_30p.txt', 'w') as file:\n",
    "    # Iterate over the list and write each string to the file\n",
    "    for string in docking_data_30p:\n",
    "        file.write(string)\n",
    "        file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspended-orientation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "stuck-preference",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "file = open('/mnt/prj/AJ/dock_bert/Data/docking_data_30p.txt', 'r')\n",
    "\n",
    "# Read all lines of the file and split them by newline characters\n",
    "lines = file.readlines()\n",
    "\n",
    "# Close the file\n",
    "file.close()\n",
    "\n",
    "# Print the lines\n",
    "#for line in lines:\n",
    "    #print(line.strip()) # strip() removes any leading or trailing whitespaces, including newlines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "inclusive-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_lines=[]\n",
    "for line in lines:\n",
    "    new_lines.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "reverse-glasgow",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Example list of data\n",
    "#data = [\"example_1\", \"example_2\", \"example_3\", \"example_4\", \"example_5\", \"example_6\", \"example_7\", \"example_8\", \"example_9\", \"example_10\"]\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(docking_data_30p)\n",
    "\n",
    "# Split the data into train, validation, and test sets\n",
    "num_examples = len(docking_data_30p)\n",
    "num_train = int(num_examples * 0.7)  # 70% for training\n",
    "num_val = int(num_examples * 0.2)  # 20% for validation\n",
    "num_test = num_examples - num_train - num_val  # rest for test\n",
    "\n",
    "train_data = docking_data_30p[:num_train]\n",
    "val_data = docking_data_30p[num_train:num_train+num_val]\n",
    "test_data = docking_data_30p[num_train+num_val:]\n",
    "\n",
    "# Save train, validation, and test sets to text files\n",
    "with open(\"/mnt/prj/AJ/dock_bert/Data/train_word_0502.txt\", \"w\") as f:\n",
    "    for example in train_data:\n",
    "        f.write(example + \"\\n\")\n",
    "\n",
    "with open(\"/mnt/prj/AJ/dock_bert/Data/val_word_0502.txt\", \"w\") as f:\n",
    "    for example in val_data:\n",
    "        f.write(example + \"\\n\")\n",
    "\n",
    "with open(\"/mnt/prj/AJ/dock_bert/Data/test_word_0502.txt\", \"w\") as f:\n",
    "    for example in test_data:\n",
    "        f.write(example + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-example",
   "metadata": {},
   "source": [
    "## Create tokenizer and train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-latino",
   "metadata": {},
   "source": [
    "### import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "second-chuck",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsml/.conda/envs/0418_test/lib/python3.6/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "import json\n",
    "\n",
    "from tokenizers import decoders, models, normalizers, pre_tokenizers, processors, trainers\n",
    "from transformers import BertTokenizerFast\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "from transformers import TrainingArguments\n",
    "from transformers import DataCollatorForWholeWordMask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fitted-trace",
   "metadata": {},
   "source": [
    "### Tokenizer Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "musical-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(unk_token=\"[UNK]\"))\n",
    "#tokenizer.add_special_tokens([\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]) # add mask token\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.BertPreTokenizer()\n",
    "special_tokens = ['[UNK]', \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\n",
    "trainer = trainers.WordLevelTrainer(vocab_size=len(set(word_all)), special_tokens=special_tokens)\n",
    "tokenizer.train(files=[\"/mnt/prj/AJ/dock_bert/Data/docking_data_30p.txt\"], trainer=trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "metropolitan-gauge",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"my_tokenizer_0502.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dedicated-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/mnt/prj/AJ/dock_bert/scripts/my_tokenizer_0502.json\", pad_token=\"[PAD]\" ,mask_token=\"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "retired-bunch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14CCCCLCD1CBCD1CD2 23COCCGCANCAC 22OCOCGCANCAC 11OCOCVCG2CG1CG2CA 14NCNCVCG2CG1CG2CA 26CCCCVCG1CACG1CG2 11OCOCKCDCGCDCE 16CCCCKCDCGCDCE 13COCCRCGCBCGCD 23COCCPCDNCDCG 13COCCVCG2CG1CG2CA 12OCOCVCG2CG1CG2CA 24NCNCDOD2OD1OD2CB 14CCCCDOD2OD1OD2CB 14CNCCDOD2OD1OD2CB 23COCCDOD2OD1OD2CB 11OCOCEOE1CGOE1OE2 11OCOCEOE1CGOE1OE2 26CCCCNOD1CBOD1ND2 13CNCCLCD1CBCD1CD2 26CCCNLCD2CD1CD2CB 26CCCNLCD2CD1CD2CB 14CCCCDOD1CBOD1OD2 14CCCCDOD2OD1OD2CB\n",
      "[5, 320, 1364, 750, 204, 87, 1269, 402, 2659, 3845, 501, 2920, 148, 17, 192, 561, 770, 770, 205, 349, 291, 291, 22, 17]\n"
     ]
    }
   ],
   "source": [
    "test_string = docking_data_30p[30]\n",
    "print(test_string)\n",
    "print(word_tokenizer(test_string).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "forbidden-hungary",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "word_tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/mnt/prj/AJ/dock_bert/scripts/my_tokenizer_0502.json\", pad_token=\"[PAD]\" ,mask_token=\"[MASK]\")\n",
    "data_collator = DataCollatorForWholeWordMask(tokenizer=word_tokenizer, mlm=True, mlm_probability=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-nevada",
   "metadata": {},
   "source": [
    "## Bert training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "exempt-jackson",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForMaskedLM\n",
    "scale_factor = 0.8\n",
    "config = BertConfig(\n",
    "    vocab_size=len(set(docking_data_30p)),\n",
    "    max_position_embeddings=int(768*scale_factor),\n",
    "    intermediate_size=int(2048*scale_factor),\n",
    "    hidden_size=int(512*scale_factor),\n",
    "    num_attention_heads=8,\n",
    "    num_hidden_layers=6,\n",
    "    #type_vocab_size=5,\n",
    "    hidden_dropout_prob=0.1,\n",
    "    attention_probs_dropout_prob=0.1,\n",
    "    next_sentence_prediction_loss_coef=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-filename",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-a67d212dd553ac23\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset text/default to /home/nsml/.cache/huggingface/datasets/text/default-a67d212dd553ac23/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 8861.21it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 157.82it/s]\n",
      "                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset text downloaded and prepared to /home/nsml/.cache/huggingface/datasets/text/default-a67d212dd553ac23/0.0.0/21a506d1b2b34316b1e82d0bd79066905d846e5d7e619823c0dd338d6f1fa6ad. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:04<00:00,  1.53s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('text', data_files={'train': '/mnt/prj/AJ/dock_bert/Data/train_word_0502.txt', \n",
    "                                           'test': '/mnt/prj/AJ/dock_bert/Data/test_word_0502.txt', \n",
    "                                           'eval': '/mnt/prj/AJ/dock_bert/Data/val_word_0502.txt'})\n",
    "small_train_dataset = dataset[\"train\"]\n",
    "small_eval_dataset = dataset[\"test\"]\n",
    "\n",
    "model = BertForMaskedLM(config)\n",
    "model.tokenizer = transformer_tokenizer\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return transformer_tokenizer(examples[\"text\"], max_length = 35, truncation=True, padding=False)\n",
    "\n",
    "encoded_dataset_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset_test = small_eval_dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metric(eval_pred):\n",
    "    return metric.compute(predictions=eval_pred.predictions, references=eval_pred.label_ids)\n",
    "\n",
    "training_args = TrainingArguments(\"test_trainer_bert_pre\",\n",
    "                                  num_train_epochs=1,\n",
    "                                  save_steps=10000,\n",
    "                                  per_device_train_batch_size=4,\n",
    "                                  per_device_eval_batch_size=4,\n",
    "                                  logging_steps=2000,\n",
    "                                  evaluation_strategy=\"steps\"\n",
    "                                     # prediction_loss_only=True,\n",
    ")\n",
    "from transformers import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-backup",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model, \n",
    "    tokenizer=transformer_tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    args=training_args, train_dataset=encoded_dataset_train,\n",
    "    eval_dataset=encoded_dataset_test\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "virtual-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-determination",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equipped-morgan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "passing-platinum",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "seven-sentence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Dock_GPT2",
   "language": "python",
   "name": "dock_gpt2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
